%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Journal Article
% LaTeX Template
% Version 1.3 (9/9/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

%\documentclass{aastex}  % version 5.0 or prior
%\usepackage{natbib}



\usepackage{graphicx}
\usepackage{lipsum} % Package to generate dummy text throughout this template
%\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[margin=1in,columnsep=20pt]{geometry} % Document margins
\usepackage{multicol} % Used for the two-column layout of the document
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables
\usepackage{float} % Required for tables and figures in the multi-column environment - they need to be placed in specific locations with the [H] (e.g. \begin{table}[H])
\usepackage{hyperref} % For hyperlinks in the PDF
\usepackage{subcaption}

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text
\usepackage{paralist} % Used for the compactitem environment which makes bullet points with less space between them
\usepackage{amsmath}
\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
%\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
%\renewcommand\thesubsection{\Roman{subsection}} % Roman numerals for subsections
%\renewcommand\thesubsubsection{\Alph{subsubsection}} % Roman numerals for subsections
\titleformat{\section}[block]{\Large\scshape}{\thesection}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection}{1em}{} % Change the look of the section titles
\titleformat{\subsubsection}[block]{}{\thesubsubsection}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{Montana State University \quad $\bullet$ \quad CSCI 466 Artificial Intelligence \quad $\bullet$ \quad Group 21} % Custom header text
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\newcommand{\ve}[1]{\boldsymbol{\mathbf{#1}}}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{\vspace{-15mm}\fontsize{24pt}{10pt}\selectfont\textbf{CSCI 446 Artificial Intelligence \\ Project 2 Final Report} \\[-2mm]} % Article title
\date{\today}
\author{
\large
\textsc{Roy Smart} \and \textsc{Nevin Leh} \and \textsc{Brian Marsh}\\[2mm] % Your name
}


%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Insert title

\thispagestyle{fancy} % All pages have headers and footers

%\begin{abstract}
%We present a novel way of performing MOSES data inversions using a
%\end{abstract}

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

%\begin{multicols}{2} % Two-column layout throughout the main article text
\normalsize

\begin{abstract}
	
\end{abstract}
\section{Introduction}

Define MLA acronym here!!!!

	In a broad sense, machine learning refers to efforts to give computers the ability to learn without explicit instructions and encompasses a wide range of problems.  Classification in the realm of machine learning is a problem that can be solved using several algorithms, but the effectiveness of the algorithm depends greatly upon the specific dataset involved.  We implemented four algorithms to attempt to classify new data points into sets based upon previous information gained through “training.”  The algorithms used are k-Nearest Neighbors, Naïve Bayes, Tree Augmented Naïve Bayes (TAN), and Iterative Dichotomiser 3 (ID3).  Additionally, each algorithm is tested with five different datasets: the Wisconsin Breast Cancer Database, the Glass Identification Database, the Iris Plants Database, the Small Soybean Database, and the 1984 United States Congressional Voting Records Database.  The effectiveness of each algorithm is measured by the metrics of precision and convergence.  

\section{Datasets}

	In this project we are asked to train our MLAs on five different datasets: the Wisconsin Breast Cancer Database, the Glass Identification Database, the Iris Plants Database, the Small Soybean Database, and the 1984 United States Congressional Voting Records Database. 
	Each dataset has a different set of challenges to our MLAs, for example, the soybean dataset is really small, the cancer database is reasonably large, and the attributes in the glass dataset are not very well correlated (as we will soon see).
	
	\subsection{Discretization}
	
		Two datasets, the iris dataset and the glass dataset have continuous values. 
		We are asked to discretize these dataset to more accurately compare the different MLAs.
		For our project, we adopted \textit{binning} as our discretization scheme.
		Binning is a simple method for discretization that defines a certain number of adjacent intervals, known as bins, and then replaces points that fall within each bin by a value representative of that bin.
		
		For the data in this report the number of bins was set to 10. In principle, the ideal number of bins in terms of MLA precision could be any integer larger than one, but through tests we found that the ideal number of bins is proportional to the number of classes in the dataset.
	
	\subsection{Cross-Validation}
	
		Cross-validation is statistical bootstrapping techinque used to estimate the generality of a statistical model using a dataset independent of the dataset used to create the model. 
		In the context fo machine learning, this means that we will partition the full datasets into training and validation datasets. 
		For this project we are asked to use the so-called $k$-fold cross validation technique, where the full dataset is partitioned into training and testing datasets through the use of \textit{folds}. 
		The folds are found by splitting the dataset into $k$ equal sized pieces, and assigning each to be a fold. 
		One fold is selected as the validation set, and the rest of the folds are assigned to the training set. 
		This process is then repeated $k$ times, where each fold is taken to be the validation set once.
	
		To further increase the accuracy of our cross-validation process, we will \textit{stratify} the folds described by $k$-fold cross-validation technique. 
		Stratification in this context means that each fold will have an equal distribution of the classes in each dataset..
		This process makes cross-validation more accurate, because it gives each MLA adequate information on each class for every training dataset.
		Without stratification, some training datasets could give a MLA zero examples of a particular class, hurting its ability to generalize.
		
		In $k$-fold cross-validation, $k$ is a free parameter that can be tuned by the user. For this project, we have selected $k = 10$, because it will allow us to perform the convergence tests discussed in Section \ref{conv} over a larger range.
	
	\subsection{Missing Values}
	
		Some of the datasets have missing values. 
		This is problematic since most of the MLA's will yield vastly different answers if some data is missing. 
		However, we note that the appearance of missing values in the voting dataset is merely an illusion, missing values in this dataset can signify a stance on a particular issue and this information should be used by our algorithms. 
		
		Since the voting set contains no missing values, the only dataset with missing values is the cancer dataset. Since there are so few missing values and since the cancer dataset is so large, we have decided to simply delete any data with missing values from the cancer dataset. Therefore, we have found that data imputation is not necessary for this project. 
	
\section{$k$-Nearest Neighbors}
	\subsection{Training}
		K-Nearest Neighbors attempts to solve the classification problem by using already-known data points with similar characteristics to make an educated guess of the class.  The training for k-Nearest Neighbors involves simply reading in all of the dataset and storing these records as points based on their values.
		\subsubsection{Constructing Probability Table}
	\subsection{Validation}
		\subsubsection{Value Distance Metric}
		\subsubsection{Determining $k$ and $p$}
		
			\begin{figure}[h!]
				\centering
				\begin{subfigure}[b]{0.32\textwidth}
					\centering
					\includegraphics[width=\textwidth]{figs/kNN/cancer_plot_kp}	
					\caption{}
					\label{kp_cancer}				
				\end{subfigure}	\	
				\begin{subfigure}[b]{0.32\textwidth}
					\centering
					\includegraphics[width=\textwidth]{figs/kNN/glass_plot_kp}
					\caption{}
					\label{kp_glass}					
				\end{subfigure} \	
				\begin{subfigure}[b]{0.32\textwidth}
					\centering
					\includegraphics[width=\textwidth]{figs/kNN/iris_plot_kp}	
					\caption{}
					\label{kp_iris}				
				\end{subfigure} \	
				\begin{subfigure}[b]{0.32\textwidth}
					\centering
					\includegraphics[width=\textwidth]{figs/kNN/soybean_plot_kp}	
					\caption{}
					\label{kp_soybean}				
				\end{subfigure} \	
				\begin{subfigure}[b]{0.32\textwidth}
					\centering
					\includegraphics[width=\textwidth]{figs/kNN/vote_plot_kp}	
					\caption{}
					\label{kp_vote}				
				\end{subfigure} \	
				\begin{subfigure}[b]{0.32\textwidth}
					\centering
					\includegraphics[width=\textwidth]{figs/kNN/average_plot_kp}	
					\caption{}
					\label{kp_ave}				
				\end{subfigure}
				\caption{}
				\label{kp_plot}
			\end{figure}
		
\section{Naive Bayes}
	\label{nbw}

	\textit{Naive Bayes} uses the principle of Bayesian learning to solve the classification problem. 
	This algorithm is said to be naive because it considers the attributes to be conditionally independent when performing classifications. 
	Under this scheme, the probability distribution of each class $C$ is given the set of attributes $x_1,...,x_n$ is written by \cite{ai} as
	
	\begin{equation}					
		\ve{P}(C|x_1,...,x_n) = \alpha \ve{P}(C) \prod_i \ve{P}(x_i| C)
		\label{nb_pd}
	\end{equation}
	
	where, using the maximum likelihood hypothesis, $\ve{P}(C)$ is the prior probability of the class $C$ in the training dataset, $\ve{P}(x_i|C)$ is the likelihood of attribute $x_i$ given $C$, and $\alpha$ is a normalization constant. 
	To find $\ve{P}(C)$ the we simply say it is equal to the proportion of $C$ observed in the training dataset. 
	Similarly, we can calculate the likelihood using the definition of conditional probability
	
	\begin{equation*}
		P(a|b) = \frac{P(a \land b)}{P(b)}
	\end{equation*}
	
	where the probabilities are calculated using the number of occurrences in the training dataset. 

	\subsection{Training}
		\label{nb_train}
		
		\begin{figure}[h!]
			\centering
			\begin{subfigure}[b]{0.4\textwidth}
				\centering
				\includegraphics[width=\textwidth]{figs/NaiveBayes/plot_ptable_cancer}	
				\caption{}
				\label{p_cancer}				
			\end{subfigure}	\	
			\begin{subfigure}[b]{0.4\textwidth}
				\centering
				\includegraphics[width=\textwidth]{figs/NaiveBayes/plot_ptable_glass}
				\caption{}
				\label{p_glass}					
			\end{subfigure} \	
			\begin{subfigure}[b]{0.4\textwidth}
				\centering
				\includegraphics[width=\textwidth]{figs/NaiveBayes/plot_ptable_iris}	
				\caption{}
				\label{p_iris}				
			\end{subfigure} \	
			\begin{subfigure}[b]{0.4\textwidth}
				\centering
				\includegraphics[width=\textwidth]{figs/NaiveBayes/plot_ptable_soybean}	
				\caption{}
				\label{p_soybean}				
			\end{subfigure} \	
			\begin{subfigure}[b]{0.4\textwidth}
				\centering
				\includegraphics[width=\textwidth]{figs/NaiveBayes/plot_ptable_vote}	
				\caption{}
				\label{p_vote}				
			\end{subfigure}
			\caption{3D density plots of the naive Bayes probability table for all five datasets. The vertical class axis represents the value of the class, $c$, the attribute axis denotes the index of an attribute, $x$, and the value axis represents the vale of that attribute, $a$. Finally the color is equivalent to the probability of the attibute $x$ having value $a$, and the class having the value $c$}
			\label{ptable}
		\end{figure}
			
		During the training phase for naive Bayes, the algorithm simply counts the how many times a specific attribute takes on a particular value for each class. 
		From these counts we build the probability table visualized in Figure \ref{ptable}. 
		In Figure \ref{ptable}, we have plotted the class vs. attribute index vs. attribute value vs. probability (in color) for each of the five datasets. 
		These plots are interesting because they serve to visualize the relationships between classes and attribute values. 
			
		If we contemplate the plots in Figure \ref{ptable}, we can start to visualize how Naive Bayes works. 
		For the cancer dataset in Figure \ref{p_cancer}, we can see how benign (class = 1) results are characterized by low values for every attribute. 
		In the glass dataset, Figure \ref{p_glass}, the low classes (building and vehicle windows) are very well characterized by the attributes, while the miscellaneous glass types (containers, tableware, headlamps) were not dependent upon the attributes. 
		In the iris dataset (Figure \ref{p_iris}), we can see beautiful regions that characterize each species. 
		The soybean dataset in Figure \ref{p_soybean} demonstrates a probability table with an interesting geometric structure, while in the voting dataset, Figure \ref{p_vote}, it seems almost as if we can see the party line between Democrats and Republicans.
		

		
	\subsection{Validation}
		
		To validate our naive Bayes classifier, we simply use the probability table discussed in Section \ref{nb_train} and Equation \ref{nb_pd} to find the most probable class. To evaluate Equation \ref{nb_pd}, our implementation looped over the classes to construct the product of the conditional probabilities. $P(C)$ was calculated by taking the total occurrences of each class and dividing by the total size of the dataset. $P(x_i | C)$ was found by taking the number of times an attribute $i$ with a specified class $C$ had value $x_i$  divided by the total number occurrences of class $C$;
		
\section{TAN}

		\textit{Tree-Augmented Naive Bayes} (TAN) is an extension to the naive Bayes algorithm described in Section \ref{nbw}. 
		Developed by \cite{Friedman1997}, TAN relaxes the assumption that the attributes are conditionally independent and allows each attribute to depend on only one other attribute. 
		It accomplishes this by constructing a fully-connected, undirected graph out of the attributes in the dataset. 
		TAN then assigns a weight to each connection in the graph using the \textit{conditional mutual information function}, given by
		
		\begin{equation*}
			I_P(\ve{X};\ve{Y} | \ve{Z}) = \sum_{\ve{x},\ve{y},\ve{z}} P(\ve{x},\ve{y}, \ve{z}) \log \left( \frac{P(\ve{x},\ve{y} | \ve{z})}{P(\ve{x}|\ve{z})P(\ve{y}|\ve{z})} \right).
			\label{cmi}
		\end{equation*}
		
		Applying the this weight to the edges allows us to construct a maximum spanning tree of the graph. 
		We can then make the spanning tree directed by choosing a root node and setting the direction of all edges to be outward from it.
		
		Using this representation, we can calculate the analog of Equation \ref{nb_pd}, the probability distribution, using the expression given by \cite{Zheng2010}
		class, attribute 1 index, attribute 1 value, attribute 2 index, attribute 2 value, count
		\begin{equation}
			\text{class} = \text{argmax}_C \left[ P(C) P(x_r | C) \prod_{x_i,x_j} P(x_j | x_i , C) \right]
		\end{equation}
		
		where $x_i$ is the parent of the attribute $x_j$ in the directed minimum spanning tree. 

	\subsection{Training}
		
		To accomplish the TAN algorithm, we will need to construct a probability table similar to the one described in Section \ref{nbw} on naive Bayes. For this algorithm however, the 4D (class, attribute index, attribute value, count) probability table of naive Bayes needs to be extended to a 6D probability table where the dimensions are: class, attribute 1 index, attribute 1 value, attribute 2 index, attribute 2 value, count, to represent probabilites of the form $P(x_i,,C)$
		
	\subsection{Validation}
		\subsubsection{Determining Class Probability Distribution}
\section{ID3}
	\subsection{Training}
	\subsubsection{Tree Construction}
	The \texttt{id3} method was the main logic behind tree construction. We closely followed the decision tree learning psuedocode in Figure 18.5 in \cite{ai}. 
	ID3 is an recursive process that generates a short tree by splitting the tree one attribute at a time. 
	The first attribute to be used is the one with the most information gain. 
	Information gain can described as how much entropy the system lost by splitting on an attribute. 
	The equation for entropy is as follows:
	\begin{equation*}
		H(S) = \sum_{i=1}^{n} \left( -p_i \log_2 p_i \right). 
	\end{equation*}		
	where $p_i$ is the proportion of the number of datums in class $i$ to the total number of datums in the set $S$. 
	To use this equation the entropy of the whole system is determined first. 
	Then the sum of the entropy for each new branch is calculated and subtracted from the total entropy. We try to branch on each attribute that has not been used yet. Since the goal is to maximize this difference so the tree will be small and more general, we choose the attribute that resulted in the greatest reduction of entropy. The reduction of entropy is defined as information gain.
	The rest of the algorithm is just iterating through the tree as it is made and recursively perform the calculation on an attribute that has not been split yet. 
	The tree is finished when all leaf nodes are a single class or when attributes to split on have run out.
	In this case each leaf node that does not have a class is assigned the most common class in that leaf.
	\subsubsection{Pruning}
	We implemented reduced error pruning to help make the tree more general. The approach was to split the training set into a smaller training set and a validation set. The tree was then generated as usual using the new training set. 
	\subsection{Validation}
\section{Results}
	\subsection{Algorithm Convergence}
		\label{conv}
		\begin{figure}[h]
			\centering
			\begin{subfigure}[b]{0.49\textwidth}
				\centering
				\includegraphics[width=\textwidth]{figs/plot_conv_cancer}					
			\end{subfigure}	\	
			\begin{subfigure}[b]{0.49\textwidth}
				\centering
				\includegraphics[width=\textwidth]{figs/plot_conv_glass}					
			\end{subfigure} \	
			\begin{subfigure}[b]{0.49\textwidth}
				\centering
				\includegraphics[width=\textwidth]{figs/plot_conv_iris}					
			\end{subfigure} \	
			\begin{subfigure}[b]{0.49\textwidth}
				\centering
				\includegraphics[width=\textwidth]{figs//plot_conv_soybean}					
			\end{subfigure} \	
			\begin{subfigure}[b]{0.49\textwidth}
				\centering
				\includegraphics[width=\textwidth]{figs/plot_conv_vote}					
			\end{subfigure}
			\caption{}
			\label{conv_plot}
		\end{figure}
	\subsection{Algorithm Precision}
\section{Conclusion}
	

	\pagebreak


	%\bibliographystyle{apj}
	\bibliographystyle{apalike}
	
	\bibliography{sources}
\end{document}
