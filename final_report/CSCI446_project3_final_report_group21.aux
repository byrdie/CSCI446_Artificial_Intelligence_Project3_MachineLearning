\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Datasets}{1}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Discretization}{1}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Cross-Validation}{2}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Missing Values}{2}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}$k$-Nearest Neighbors}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Training}{2}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Validation}{2}{subsection.3.2}}
\newlabel{mdm}{{1}{2}{Validation}{equation.3.1}{}}
\citation{wilson1996}
\citation{ai}
\newlabel{vdm}{{2}{3}{Validation}{equation.3.2}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{kp_cancer}{{1a}{3}{\relax }{figure.caption.1}{}}
\newlabel{sub@kp_cancer}{{a}{3}{\relax }{figure.caption.1}{}}
\newlabel{kp_glass}{{1b}{3}{\relax }{figure.caption.1}{}}
\newlabel{sub@kp_glass}{{b}{3}{\relax }{figure.caption.1}{}}
\newlabel{kp_iris}{{1c}{3}{\relax }{figure.caption.1}{}}
\newlabel{sub@kp_iris}{{c}{3}{\relax }{figure.caption.1}{}}
\newlabel{kp_soybean}{{1d}{3}{\relax }{figure.caption.1}{}}
\newlabel{sub@kp_soybean}{{d}{3}{\relax }{figure.caption.1}{}}
\newlabel{kp_vote}{{1e}{3}{\relax }{figure.caption.1}{}}
\newlabel{sub@kp_vote}{{e}{3}{\relax }{figure.caption.1}{}}
\newlabel{kp_ave}{{1f}{3}{\relax }{figure.caption.1}{}}
\newlabel{sub@kp_ave}{{f}{3}{\relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A series of plots demonstrating the precision (represented by color) of $k$ nearest-neighbors for different values of $k$ and the $p$ value in the Minkowski distance. We have plotted the response of kNN for all five datasets (Figure \ref  {kp_cancer} - \ref  {kp_vote}) and the average response over all datasets, Figure \ref  {kp_ave}.\relax }}{3}{figure.caption.1}}
\newlabel{kp_plot}{{1}{3}{A series of plots demonstrating the precision (represented by color) of $k$ nearest-neighbors for different values of $k$ and the $p$ value in the Minkowski distance. We have plotted the response of kNN for all five datasets (Figure \ref {kp_cancer} - \ref {kp_vote}) and the average response over all datasets, Figure \ref {kp_ave}.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Naive Bayes}{4}{section.4}}
\newlabel{nbw}{{4}{4}{Naive Bayes}{section.4}{}}
\newlabel{nb_pd}{{3}{4}{Naive Bayes}{equation.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Training}{4}{subsection.4.1}}
\newlabel{nb_train}{{4.1}{4}{Training}{subsection.4.1}{}}
\newlabel{p_cancer}{{2a}{4}{\relax }{figure.caption.2}{}}
\newlabel{sub@p_cancer}{{a}{4}{\relax }{figure.caption.2}{}}
\newlabel{p_glass}{{2b}{4}{\relax }{figure.caption.2}{}}
\newlabel{sub@p_glass}{{b}{4}{\relax }{figure.caption.2}{}}
\newlabel{p_iris}{{2c}{4}{\relax }{figure.caption.2}{}}
\newlabel{sub@p_iris}{{c}{4}{\relax }{figure.caption.2}{}}
\newlabel{p_soybean}{{2d}{4}{\relax }{figure.caption.2}{}}
\newlabel{sub@p_soybean}{{d}{4}{\relax }{figure.caption.2}{}}
\newlabel{p_vote}{{2e}{4}{\relax }{figure.caption.2}{}}
\newlabel{sub@p_vote}{{e}{4}{\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces 3D density plots of the naive Bayes probability table for all five datasets. The vertical class axis represents the value of the class, $c$, the attribute axis denotes the index of an attribute, $x$, and the value axis represents the vale of that attribute, $a$. Finally the color is equivalent to the probability of the attibute $x$ having value $a$, and the class having the value $c$\relax }}{4}{figure.caption.2}}
\newlabel{ptable}{{2}{4}{3D density plots of the naive Bayes probability table for all five datasets. The vertical class axis represents the value of the class, $c$, the attribute axis denotes the index of an attribute, $x$, and the value axis represents the vale of that attribute, $a$. Finally the color is equivalent to the probability of the attibute $x$ having value $a$, and the class having the value $c$\relax }{figure.caption.2}{}}
\citation{Friedman1997}
\citation{Zheng2010}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Validation}{5}{subsection.4.2}}
\newlabel{nbv}{{4.2}{5}{Validation}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}TAN}{5}{section.5}}
\newlabel{cmi}{{5}{5}{TAN}{section.5}{}}
\newlabel{tan_pd}{{4}{5}{TAN}{equation.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Training}{5}{subsection.5.1}}
\citation{kruskal}
\citation{ai}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Example of the maximum spanning tree produced by TAN on the cancer dataset.\relax }}{6}{figure.caption.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Validation}{6}{subsection.5.2}}
\@writefile{toc}{\contentsline {section}{\numberline {6}ID3}{7}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Training}{7}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Tree Construction}{7}{subsubsection.6.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces An example id3 tree generated for the soybean database.\relax }}{7}{figure.caption.4}}
\newlabel{tan}{{4}{7}{An example id3 tree generated for the soybean database.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Pruning}{7}{subsubsection.6.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Validation}{8}{subsection.6.2}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Results}{8}{section.7}}
\newlabel{conv}{{7}{8}{Results}{section.7}{}}
\newlabel{conv_cancer}{{5a}{8}{\relax }{figure.caption.5}{}}
\newlabel{sub@conv_cancer}{{a}{8}{\relax }{figure.caption.5}{}}
\newlabel{conv_glass}{{5b}{8}{\relax }{figure.caption.5}{}}
\newlabel{sub@conv_glass}{{b}{8}{\relax }{figure.caption.5}{}}
\newlabel{conv_iris}{{5c}{8}{\relax }{figure.caption.5}{}}
\newlabel{sub@conv_iris}{{c}{8}{\relax }{figure.caption.5}{}}
\newlabel{conv_soybean}{{5d}{8}{\relax }{figure.caption.5}{}}
\newlabel{sub@conv_soybean}{{d}{8}{\relax }{figure.caption.5}{}}
\newlabel{conv_vote}{{5e}{8}{\relax }{figure.caption.5}{}}
\newlabel{sub@conv_vote}{{e}{8}{\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Plot showing the learning curves for each of the algorithms on the five datasets. Please note the low point on the right hand side of Figures \ref  {conv_cancer}, \ref  {conv_glass}, \ref  {conv_soybean},\ref  {conv_vote} is in error and is not part of our results.\relax }}{8}{figure.caption.5}}
\newlabel{conv_plot}{{5}{8}{Plot showing the learning curves for each of the algorithms on the five datasets. Please note the low point on the right hand side of Figures \ref {conv_cancer}, \ref {conv_glass}, \ref {conv_soybean},\ref {conv_vote} is in error and is not part of our results.\relax }{figure.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces 1x10 Cross Validation Precision. Green represents the best performing algorithm for each data set. Red represents the worst.\relax }}{9}{table.caption.6}}
\newlabel{table}{{1}{9}{1x10 Cross Validation Precision. Green represents the best performing algorithm for each data set. Red represents the worst.\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Cancer Dataset Performance}{9}{subsection.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Glass Dataset Performance}{9}{subsection.7.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Iris Dataset Performance}{9}{subsection.7.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Soybean Dataset Performance}{9}{subsection.7.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Voting Dataset Performance}{9}{subsection.7.5}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{9}{section.8}}
\bibstyle{apalike}
\bibdata{sources}
\bibcite{Friedman1997}{Friedman et\nobreakspace  {}al., 1997}
\bibcite{kruskal}{Kruskal, 1956}
\bibcite{ai}{Russel and Norvig, 2010}
\bibcite{wilson1996}{Wilson and Martinez, 1996}
\bibcite{Zheng2010}{Zheng and Webb, 2010}
